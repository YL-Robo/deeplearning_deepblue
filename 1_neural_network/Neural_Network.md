# 第一部分：极大似然估计与逻辑回归

**(PDF Page 3 - 16)**

### 1. 理论核心

**1.1 什么是极大似然估计 (MLE)?**

* **定义**：利用已知的样本结果（观测值），反推最有可能导致这些结果出现的模型参数 。


* **核心区别**：
* **概率函数 (Probability)**：参数已知  预测数据发生的可能性 。


* **似然函数 (Likelihood)**：数据已知  推测参数的可能性 。


* *通俗理解*：如果是“已知黑球白球比例算抓到白球的概率”，这是概率问题；如果是“抓了一把球看颜色，反推桶里黑白比例”，这是极大似然问题。



**1.2 逻辑回归 (Logistic Regression)**

* **模型**：虽然叫回归，但本质是**分类算法** 。


* **核心公式**：
使用 **Sigmoid 函数** 将线性输出  压缩到  之间，表示概率：







* **损失函数**：对数似然损失（交叉熵）。为了求导方便，我们对似然函数取对数并取反（求极小值）：





### 2. 辅助工具与代码

为了理解 Sigmoid 函数如何将数值“挤压”成概率，我们可以运行以下代码：

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 100)
plt.plot(z, sigmoid(z))
plt.title("Sigmoid Function: Mapping (-inf, inf) to (0, 1)")
plt.grid(True)
plt.show()

```

### 3. 课后作业 I & II 解答

**(PDF Page 8, 15)**

**作业 I (Page 8)**

* **题1**：数据已知（抽样100次70白），求参数（白球比例）。这是由果推因。
* **答案**：B. 似然函数 



* **题2**：参数已知（白球占比70%），求数据发生的可能性。这是由因推果。
* **答案**：A. 概率函数 


**作业 II (Page 15)**

* **问题**：证明为什么负梯度方向是损失函数下降的方向？
* **解答**：
**1. 泰勒展开推导** 设损失函数为 $f(\mathbf{x})$，当变量有一个微小增量  时，根据**一阶泰勒展开**，函数值的变化量  可以近似表示为：



为了使函数值下降，必须保证 ，即：


**2. 向量点积分析** 根据向量点积公式，上述条件可以改写为：



其中  是梯度向量  与增量向量  之间的夹角：
* **下降条件**：只要 （即夹角大于 ），函数值就会下降。
* **最快下降**：当  时，，此时  与梯度方向**完全相反**。


**3. 结论** 为了让损失函数  下降得最快，增量  应当取负梯度方向。因此，参数更新步长可以表示为 ，其中  为学习率。

---

# 第二部分：感知机 (Perceptron)

**(PDF Page 17 - 25)**

### 1. 理论核心

* **定义**：感知机是神经网络的基石，用于解决**线性可分**问题 。


* **模型**：



(输出为 +1 或 -1)
* **学习策略**：
不是最小化分类错误的个数（因为不可导），而是最小化**误分类点到超平面的总距离** 。


* **更新规则**：
一旦发现误分类点 ，就调整  和  让超平面向该点移动 ：






### 2. 辅助工具：感知机迭代逻辑

感知机无法解决非线性问题（如异或 XOR），这是它最大的局限。

### 3. 课后作业 III 解答

**(PDF Page 25)**

**题目**：补全感知机迭代表格。
已知：正样本 ，负样本 。
当前状态 (第4步结束)：。
**第5步**：选择  作为误分类点。

* **计算验证**：
代入 ，计算 ：
 (确实误分类)。
* **参数更新**：


**当前超平面**：
* **检查  (负样本 )**：
代入新参数： (分类正确)。
*注意：之前步骤中M3分类错误，但现在我们看第6步是否还需要更新。如果M3算出来结果是对的，那通常需要检查所有点。但在作业逻辑中，如果M3仍被视为误分类，则继续更新。让我们检查M3的值：，判为正类，但M3是负类。所以M3此时是误分类点。*

**第6步**：选择  为误分类点。

* **参数更新**：


**最终超平面**： (即 )。

---

# 第三部分：神经网络与反向传播

**(PDF Page 26 - 57)**

### 1. 理论核心

**1.1 解决非线性 (Non-linearity)**

* **异或问题 (XOR)**：感知机解决不了 XOR 问题，因为找不到一条直线能分开它 。


* **多层网络**：通过增加隐藏层，神经网络可以对输入空间进行扭曲、折叠（非线性变换），从而解决复杂分类 。



**1.2 神经网络结构**

* **神经元计算**：输入  线性变换 ()  激活函数 () 。


* **正向传播**：数据从输入层一层层传到输出层 。



**1.3 反向传播 (Backpropagation)**

* **核心思想**：利用**链式法则**，从输出层开始，将误差 () 一层层向回传递，计算每一层参数的梯度 。


* **关键变量 **：表示误差项。
* 输出层误差： 。


* 隐藏层误差： 。


* *解释*：每一层的误差 = (上一层传回来的误差权重和)  (当前层的激活函数导数)。



### 2. 辅助代码：手写神经网络 (XOR问题)

这是本章的终极工具，用于理解多层网络如何通过 BP 算法学习。

```python
import numpy as np

# 数据: XOR
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])

# 激活函数
def sigmoid(x): return 1 / (1 + np.exp(-x))
def sigmoid_deriv(x): return x * (1 - x)

# 初始化参数
input_size, hidden_size, output_size = 2, 4, 1
# 对应 PDF 中的 Theta
W1 = np.random.uniform(-1, 1, (input_size, hidden_size)) 
W2 = np.random.uniform(-1, 1, (hidden_size, output_size))

# 训练循环 (BP算法)
lr = 0.5
for i in range(5000):
    # --- 前向传播 (PDF Page 36) ---
    z1 = np.dot(X, W1)
    a1 = sigmoid(z1) # 隐藏层激活值
    z2 = np.dot(a1, W2)
    output = sigmoid(z2) # 输出层预测值
    
    # --- 反向传播 (PDF Page 48-50) ---
    error = y - output
    # 输出层误差 delta^(L)
    d_output = error * sigmoid_deriv(output) 
    # 隐藏层误差 delta^(l) = (delta^(l+1) * W) * f'
    d_hidden = d_output.dot(W2.T) * sigmoid_deriv(a1)
    
    # --- 更新权重 (Gradient Descent) ---
    W2 += a1.T.dot(d_output) * lr
    W1 += X.T.dot(d_hidden) * lr

print("XOR 预测结果:\n", output)

```

### 3. 课后作业 IV & V 解答

**(PDF Page 39, 57)**

**作业 IV (Page 39)**

* **Q1: 为什么多层神经网络可以拟合任意函数？**
* **A**: 基于**通用近似定理**。只要有非线性激活函数，且隐藏层神经元足够多，神经网络可以通过组合无数个小的非线性变换来逼近任何连续函数 。




* **Q2: 深层网络 vs 浅层宽网络？**
* **A**: 浅层宽网络参数效率低，容易过拟合。深层网络通过层级结构（Layer-wise）提取特征（如边缘$\rightarrow\rightarrow$物体），参数利用率更高，泛化能力更强。



**作业 V (Page 57)**

* **Q1: 参数统计 (MNIST案例)**
* 输入784，隐藏100，输出10。
* 计算： 。




* **Q2: 图片变大 (256x256) 的影响**
* 输入层变为 。第一层权重变为  万。参数量剧增，导致计算变慢且极易过拟合。


* **Q3: 降低复杂度的措施**
* 不使用全连接网络，改用**卷积神经网络 (CNN)**，利用**局部连接**和**权值共享**来大幅减少参数量。